{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "513ff275-794f-4fb9-a4cc-d6c8d6bad52b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark Structured Streaming with Iceberg: bronze to silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ba2f8ef-b23a-4b02-9e8f-4ae5be8fa167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this notebook, we will read messages from Pulsar, then ingest to bronze Iceberg table, then readStream from the bronze Iceberg bronze table and write to other downstream tables. \n",
    "\n",
    "Considerations:\n",
    "- The bronze table allows persisting all message so other downstream tables can be processed without losing messing\n",
    "- Allow replayability when raw messages are saved in Iceberg tables to reprocess\n",
    "- However, we cannot identify the latest changes readStream from the Iceberg table, so the spark.readStream(bronze_table) will have all the data in the table, potential perfomance issue with very large bronze table. The write to downstream table has checkpoint so we can still guarantee idempotent write "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Databricks! Spark version: 3.5.2\n",
      "+--------------------+\n",
      "|        current_time|\n",
      "+--------------------+\n",
      "|2025-07-23 01:23:...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use Databricks Connect to connect to Databricks and run notebook interactively from IDE on local machine, comment this cell if running on Databricks workspace\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "cluster_id = \"0709-132523-cnhxf2p6\"\n",
    "\n",
    "# Create Databricks Connect session using profile\n",
    "spark = DatabricksSession.builder \\\n",
    "    .profile(\"DEFAULT\") \\\n",
    "    .remote(cluster_id=cluster_id) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✅ Connected to Databricks! Spark version: {spark.version}\")\n",
    "\n",
    "# Test the connection\n",
    "df = spark.sql(\"SELECT current_timestamp() as current_time\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dbc4ca0-f2c9-40dd-a034-a672fd89689c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_url = \"pulsar://44.247.85.233:6650\"\n",
    "topic = \"financial-messages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schemas import schema, instrument_ref_schema, instrument_error_schema, instrument_risk_schema \n",
    "from pyspark.sql.functions import col, from_json, schema_of_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f752159-5300-4dbc-86b6-269ca6f6c477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fin_df = (\n",
    "    spark.readStream\n",
    "    .format(\"pulsar\")\n",
    "    .option(\"service.url\", service_url)\n",
    "    .option(\"topics\", topic)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    "    .select(\"value.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fin_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c4ea17-6399-42f0-b76d-7978c607f3bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Iceberg tables need to be created before writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8908198917773943,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "747c2821-d032-455a-9cc6-e1edba3d85cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users.anhhoang_chu.bronze_fin_instrument ( {schema} )\n",
    "USING ICEBERG\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8908198917773943,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d404b950-5aee-4d13-a85a-8df8aeead33d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x308aa2fc0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    fin_df.writeStream\n",
    "    .format(\"iceberg\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"schemaLocation\", \"/Volumes/users/anhhoang_chu/iceberg/bronze1/_schema\")\n",
    "    .option(\"checkpointLocation\", \"/Volumes/users/anhhoang_chu/iceberg/bronze1/_checkpoint\")\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(\"users.anhhoang_chu.bronze_fin_instrument\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8908198917773943,
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6c64762-383b-4676-8164-7e0922d78a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-07-23 01:23:58.935</td>\n",
       "      <td>2917128087014142</td>\n",
       "      <td>anhhoang.chu@databricks.com</td>\n",
       "      <td>STREAMING UPDATE</td>\n",
       "      <td>{'outputMode': 'Append', 'queryId': '8aadc9b1-d7a6-4c9f-9521-cb48e42dec4b', 'epochId': '3', 'statsOnLoad': 'false'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0709-132523-cnhxf2p6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>WriteSerializable</td>\n",
       "      <td>True</td>\n",
       "      <td>{'numRemovedFiles': '0', 'numAddedFiles': '0', 'numOutputRows': '0', 'numOutputBytes': '0'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Databricks-Runtime/16.4.x-photon-scala2.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-07-22 22:22:27.175</td>\n",
       "      <td>2917128087014142</td>\n",
       "      <td>anhhoang.chu@databricks.com</td>\n",
       "      <td>STREAMING UPDATE</td>\n",
       "      <td>{'outputMode': 'Append', 'queryId': '8aadc9b1-d7a6-4c9f-9521-cb48e42dec4b', 'epochId': '2', 'statsOnLoad': 'false'}</td>\n",
       "      <td>None</td>\n",
       "      <td>{'notebookId': '3205547712014776'}</td>\n",
       "      <td>0722-214326-7mdrhwya</td>\n",
       "      <td>2.0</td>\n",
       "      <td>WriteSerializable</td>\n",
       "      <td>True</td>\n",
       "      <td>{'numRemovedFiles': '0', 'numAddedFiles': '1', 'numOutputRows': '1', 'numOutputBytes': '74835'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Databricks-Runtime/17.0.x-scala2.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-07-22 22:03:57.247</td>\n",
       "      <td>2917128087014142</td>\n",
       "      <td>anhhoang.chu@databricks.com</td>\n",
       "      <td>STREAMING UPDATE</td>\n",
       "      <td>{'outputMode': 'Append', 'queryId': '8aadc9b1-d7a6-4c9f-9521-cb48e42dec4b', 'epochId': '1', 'statsOnLoad': 'false'}</td>\n",
       "      <td>None</td>\n",
       "      <td>{'notebookId': '3205547712014776'}</td>\n",
       "      <td>0722-214326-7mdrhwya</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WriteSerializable</td>\n",
       "      <td>True</td>\n",
       "      <td>{'numRemovedFiles': '0', 'numAddedFiles': '1', 'numOutputRows': '1', 'numOutputBytes': '74837'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Databricks-Runtime/17.0.x-scala2.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-07-22 22:00:18.850</td>\n",
       "      <td>2917128087014142</td>\n",
       "      <td>anhhoang.chu@databricks.com</td>\n",
       "      <td>STREAMING UPDATE</td>\n",
       "      <td>{'outputMode': 'Append', 'queryId': '8aadc9b1-d7a6-4c9f-9521-cb48e42dec4b', 'epochId': '0', 'statsOnLoad': 'false'}</td>\n",
       "      <td>None</td>\n",
       "      <td>{'notebookId': '3205547712014776'}</td>\n",
       "      <td>0722-214326-7mdrhwya</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WriteSerializable</td>\n",
       "      <td>True</td>\n",
       "      <td>{'numRemovedFiles': '0', 'numAddedFiles': '1', 'numOutputRows': '11', 'numOutputBytes': '77899'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Databricks-Runtime/17.0.x-scala2.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-22 22:00:08.464</td>\n",
       "      <td>2917128087014142</td>\n",
       "      <td>anhhoang.chu@databricks.com</td>\n",
       "      <td>CREATE TABLE</td>\n",
       "      <td>{'partitionBy': '[]', 'clusterBy': '[]', 'description': None, 'isManaged': 'true', 'properties': '{\"delta.enableIcebergCompatV2\":\"true\",\"write.metadata.path\":\"s3://databricks-e2demofieldengwest/b169b504-4c54-49f2-bc3a-adf4b128f36d/tables/520e699f-4bd9-48ca-bc00-458383a89d44/_iceberg/metadata\",\"delta.universalFormat.enabledFormats\":\"iceberg\",\"write.parquet.compression-codec\":\"zstd\",\"delta.enableIcebergWriterCompatV1\":\"true\",\"write.summary.partition-limit\":\"100\",\"write.wap.enabled\":\"false\",\"delta.columnMapping.mode\":\"id\",\"delta.columnMapping.maxColumnId\":\"150\",\"history.expire.min-snapshots-to-keep\":\"100\",\"write.data.path\":\"s3://databricks-e2demofieldengwest/b169b504-4c54-49f2-bc3a-adf4b128f36d/tables/520e699f-4bd9-48ca-bc00-458383a89d44\",\"history.expire.max-snapshot-age-ms\":\"0\",\"delta.enableTypeWidening\":\"true\",\"write.metadata.compression-codec\":\"gzip\",\"delta.checkpointPolicy\":\"v2\",\"write.object-storage.enabled\":\"true\",\"gc.enabled\":\"false\",\"delta.enableInCommitTimestamps\":\"true\"}', 'statsOnLoad': 'false'}</td>\n",
       "      <td>None</td>\n",
       "      <td>{'notebookId': '3205547712014776'}</td>\n",
       "      <td>0722-214326-7mdrhwya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WriteSerializable</td>\n",
       "      <td>True</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>Databricks-Runtime/17.0.x-scala2.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "DataFrame[version: bigint, timestamp: timestamp, userId: string, userName: string, operation: string, operationParameters: map<string,string>, job: struct<jobId:string,jobName:string,jobRunId:string,runId:string,jobOwnerId:string,triggerType:string>, notebook: struct<notebookId:string>, clusterId: string, readVersion: bigint, isolationLevel: string, isBlindAppend: boolean, operationMetrics: map<string,string>, userMetadata: string, engineInfo: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql\n",
    "desc history users.anhhoang_chu.bronze_fin_instrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8908198917773943,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d401051-0bed-433d-abf2-3fedec356fb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"s3://databricks-e2demofieldengwest/b169b504-4c54-49f2-bc3a-adf4b128f36d/tables/520e699f-4bd9-48ca-bc00-458383a89d44\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db2bf487-adde-4fd0-9d88-0bf98d55d9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "907285e7-8326-4074-bcdd-d76c8e61ab41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Instrument Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8908198917773943,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "088b7c46-6459-4f44-aa4a-f1fc828624d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users.anhhoang_chu.silver_instrument_reference({instrument_ref_schema})\n",
    "USING ICEBERG\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8908198917773943,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "777f072b-b0da-4d74-89fd-9d24aa72f41d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();\ntahoe\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.throwError(UnsupportedOperationChecker.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.$anonfun$checkForBatch$2(UnsupportedOperationChecker.scala:65)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.$anonfun$checkForBatch$2$adapted(UnsupportedOperationChecker.scala:63)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:1895)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:1895)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:1895)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:1895)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:1895)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:1895)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForBatch(UnsupportedOperationChecker.scala:63)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForBatch(UnsupportedOperationChecker.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.assertSupported(QueryExecution.scala:281)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyWithCachedData$2(QueryExecution.scala:518)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyWithCachedData$1(QueryExecution.scala:516)\n\tat scala.util.Try$.apply(Try.scala:210)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:526)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getResultCacheStats(ResultCacheManager.scala:615)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.processAsArrowBatches(SparkConnectPlanExecution.scala:217)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:131)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:404)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:313)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:261)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/databricks-pulsar-iceberg/.venv/lib/python3.12/site-packages/IPython/core/formatters.py:402\u001b[39m, in \u001b[36mBaseFormatter.__call__\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[32m    404\u001b[39m method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.ipython/profile_default/startup/00-databricks-init-4345b90cdbba7425a7042b5998d089f5.py:473\u001b[39m, in \u001b[36mregister_formatters.<locals>.df_html\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdf_html\u001b[39m(df):\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnotebook_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataframe_display_limit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.to_html()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/databricks-pulsar-iceberg/.venv/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:2011\u001b[39m, in \u001b[36mDataFrame.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2009\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtoPandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mpandas.DataFrame\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2010\u001b[39m     query = \u001b[38;5;28mself\u001b[39m._plan.to_proto(\u001b[38;5;28mself\u001b[39m._session.client)\n\u001b[32m-> \u001b[39m\u001b[32m2011\u001b[39m     pdf, ei = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plan\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2012\u001b[39m     \u001b[38;5;28mself\u001b[39m._execution_info = ei\n\u001b[32m   2013\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pdf\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/databricks-pulsar-iceberg/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1274\u001b[39m, in \u001b[36mSparkConnectClient.to_pandas\u001b[39m\u001b[34m(self, plan, observations)\u001b[39m\n\u001b[32m   1270\u001b[39m (self_destruct_conf,) = \u001b[38;5;28mself\u001b[39m.get_config_with_defaults(\n\u001b[32m   1271\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mspark.sql.execution.arrow.pyspark.selfDestruct.enabled\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1272\u001b[39m )\n\u001b[32m   1273\u001b[39m self_destruct = cast(\u001b[38;5;28mstr\u001b[39m, self_destruct_conf).lower() == \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1274\u001b[39m table, schema, metrics, observed_metrics, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_destruct\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_destruct\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1278\u001b[39m ei = ExecutionInfo(metrics, observed_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/databricks-pulsar-iceberg/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1970\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch\u001b[39m\u001b[34m(self, req, observations, extra_request_metadata, self_destruct)\u001b[39m\n\u001b[32m   1967\u001b[39m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1969\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers=\u001b[38;5;28mself\u001b[39m._progress_handlers, operation_id=req.operation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m-> \u001b[39m\u001b[32m1970\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_request_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1974\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/databricks-pulsar-iceberg/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1946\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req, observations, extra_request_metadata, progress)\u001b[39m\n\u001b[32m   1944\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[32m   1945\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1946\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/databricks-pulsar-iceberg/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2266\u001b[39m, in \u001b[36mSparkConnectClient._handle_error\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m   2264\u001b[39m \u001b[38;5;28mself\u001b[39m.thread_local.inside_error_handling = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc.RpcError):\n\u001b[32m-> \u001b[39m\u001b[32m2266\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2267\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   2268\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCannot invoke RPC\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/databricks-pulsar-iceberg/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2377\u001b[39m, in \u001b[36mSparkConnectClient._handle_rpc_error\u001b[39m\u001b[34m(self, rpc_error)\u001b[39m\n\u001b[32m   2363\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(\n\u001b[32m   2364\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mPython versions in the Spark Connect client and server are different. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2365\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mTo execute user-defined functions, client and server should have the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2373\u001b[39m                         \u001b[33m\"\u001b[39m\u001b[33msqlState\u001b[39m\u001b[33m\"\u001b[39m, default=SparkConnectGrpcException.CLIENT_UNEXPECTED_MISSING_SQL_STATE),\n\u001b[32m   2374\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2375\u001b[39m             \u001b[38;5;66;03m# END-EDGE\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2377\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[32m   2378\u001b[39m                 info,\n\u001b[32m   2379\u001b[39m                 status.message,\n\u001b[32m   2380\u001b[39m                 \u001b[38;5;28mself\u001b[39m._fetch_enriched_error(info),\n\u001b[32m   2381\u001b[39m                 \u001b[38;5;28mself\u001b[39m._display_server_stack_trace(),\n\u001b[32m   2382\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2384\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(\n\u001b[32m   2385\u001b[39m         message=status.message,\n\u001b[32m   2386\u001b[39m         sql_state=SparkConnectGrpcException.CLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n\u001b[32m   2387\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2388\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAnalysisException\u001b[39m: Queries with streaming sources must be executed with writeStream.start();\ntahoe\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.throwError(UnsupportedOperationChecker.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.$anonfun$checkForBatch$2(UnsupportedOperationChecker.scala:65)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.$anonfun$checkForBatch$2$adapted(UnsupportedOperationChecker.scala:63)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:1895)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:1895)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:1895)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:1895)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:1895)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:1895)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForBatch(UnsupportedOperationChecker.scala:63)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForBatch(UnsupportedOperationChecker.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.assertSupported(QueryExecution.scala:281)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyWithCachedData$2(QueryExecution.scala:518)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyWithCachedData$1(QueryExecution.scala:516)\n\tat scala.util.Try$.apply(Try.scala:210)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:526)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getResultCacheStats(ResultCacheManager.scala:615)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.processAsArrowBatches(SparkConnectPlanExecution.scala:217)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:131)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:404)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:313)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:261)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[jobidentifier: string, analysisidentifier: string, data_item: struct<type:string,instrumentreference:struct<analysisidentifier:string,instrumentidentifier:string,asofdate:string,accountidentifier:string,accountname:string,instrumentname:string,description:string,instrumenttype:string,instrumentsubtype:string,consumerproductcategory:string,originationdate:string,maturitydate:string,amortizationtype:string,amortizationenddate:string,isinterestonly:string,cashflowtype:string,instrumentcurrency:string,notionalportion:double,unpaidprincipalbalance:string,currentcommitmentamount:double,marketpriceoverride:string,fixedpaymentamount:double,currentbookpriceoverride:string,interestratetype:string,interestpaymentfrequency:string,curerate:double,fixedrate:double,currentrate:double,portfolioidentifier:string,interestratespread:double,interestrateindexmultiplier:double,interestrateindex:string,lifetimeinterestratecap:double,lifetimeinterestratefloor:double,periodicinterestratecap:double,periodicratefloor:double,interestrateresetfirstdate:string,interestrateresetfrequency:string,daycount:string,optionadjustedspreadoverride:double,modified:string,parmarketprice:double,servicingspread:double,company:string,discountcurve:string,accountside:string,jobidentifier:string,cashfloworder:bigint,cashflowsource:string,cashflowmodelname:string,prepaymentorder:bigint,prepaymentsource:string,prepaymentmodelname:string,prepaymentshift:double,prepaymentscalingfactor:double>,instrumentriskmetric:array<struct<analysisidentifier:string,reportingdate:string,inputscenarioidentifier:string,instrumentidentifier:string,scenarioidentifier:string,modelname:string,modeloutput:string,asofdate:string,term:double,timesegment:string,annualizedcumulativepd:double,forwardpd:double,cumulativepd:double,marginalpd:double,maturityriskpd:double,maturityriskel:double,lgd:double,maturityrisklgd:double,lossrateannualized:double,lossratecumulative:double,ead:double,ccf:double,ugd:double,prepaymentrate:double,forwardprepaymentrate:double,cumulativeprepaymentrate:double,recovery:double,netchargeoff:double,annualizedpdoneyearprojection:double,stage1conditionalannualizedcumulativepd:double,stage2conditionalannualizedcumulativepd:double,stage3conditionalannualizedcumulativepd:double,impliedstagerating:string,netchargeoffamount:double,collateralvalue:double,expectedcreditlossamount:double,expectedcreditlossamountlifetimeprojection:double,expectedcreditlossamountoneyearprojection:double,exposure:double,grossinterestincome:double,totalinterestexpense:double,riskweightedassets:double,stage1portion:double,stage2portion:double,stage3portion:double,transitionprobabilityfromstage1tostage2:double,transitionprobabilityfromstage1tostage3:double,transitionprobabilityfromstage2tostage1:double,transitionprobabilityfromstage2tostage3:double,transitionprobabilityfromstage3tostage2:double,balancegrowthrate:double,lgdvariance:double,transactionsequence:bigint,creditotherthantemporaryimpairment:double,noncreditotherthantemporaryimpairment:double,temporaryimpairment:double,othercomprehensiveincome:double,otherthantemporaryimpairmentprobability:double,jobidentifier:string,valuedate:string,decayrate:double,rateresponserate:double,usagerate:double,liquidityhaircut:double,singlemonthlymortalityrate:double,edfimpliedrating:string,optionarmminimumpaymentportion:double,optionarminterestonlyportion:double,optionarmprincipalandinterestportion:double,forbearanceportion:double,forwarddecayrate:double>>,instrumentcashflow:string,instrumenttimebucketmeasures:string,instrumenterror:array<struct<analysisidentifier:string,jobidentifier:string,instrumentidentifier:string,errorcode:bigint,errormessage:string,modulecode:bigint,asofdate:string,scenarioidentifier:string,severity:string,portfolioidentifier:string>>,accounttimebucketmeasures:string,accountcashflow:string>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We cannot identify the latest changes  when readStream from the Iceberg table, so exploded_df will have all the data in the table\n",
    "\n",
    "from pyspark.sql.functions import explode, col, from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "exploded_df = (\n",
    "      spark.readStream\n",
    "        .format(\"iceberg\")\n",
    "        .table(\"users.anhhoang_chu.bronze_fin_instrument\")\n",
    "        .select(\n",
    "            col(\"jobidentifier\"),\n",
    "            col(\"analysisidentifier\"),\n",
    "            explode(col(\"data\")).alias(\"data_item\")\n",
    "        )\n",
    "        .filter(col(\"data_item.type\") == \"instrument\")\n",
    "    )\n",
    "\n",
    "display(exploded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8908198917773943,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f89a88-0df3-42fd-8b1c-81063f47706b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# However, the checkpointLocatiion allows us to only write the new records to silver table\n",
    "ref_df = (\n",
    "    exploded_df.select(\n",
    "        col(\"data_item.instrumentreference.*\")\n",
    "    )\n",
    ")\n",
    "(\n",
    "ref_df.writeStream\n",
    "    .queryName(\"silver_instrument_reference_stream\")\n",
    "    .format(\"iceberg\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"schemaLocation\", \"/Volumes/users/anhhoang_chu/iceberg/silver/ref_schema\")\n",
    "    .option(\"checkpointLocation\", \"/Volumes/users/anhhoang_chu/iceberg/silver/_ref_checkpoint\")\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(\"users.anhhoang_chu.silver_instrument_reference\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8908198917773943,
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7698c83-ecd3-4e32-a7ce-240d213aa630",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753136583534}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from users.anhhoang_chu.silver_instrument_reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e0c5920-799b-4bc6-9f42-c12d2c01e1ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Instrument Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8908198917773943,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "839a70fa-8f83-4ef8-b7ae-70c0f9ad4b30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users.anhhoang_chu.silver_instrument_error({instrument_error_schema})\n",
    "USING ICEBERG\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8908198917773943,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68cfcdf9-4818-42df-8406-cca912888eee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col \n",
    "\n",
    "error_df = exploded_df.withColumn(\"errors\", explode(col(\"data_item.instrumenterror\"))).select(\"errors.*\")\n",
    "\n",
    "(\n",
    "error_df.writeStream\n",
    "    .queryName(\"silver_instrument_error_stream\")\n",
    "    .format(\"iceberg\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"schemaLocation\", \"/Volumes/users/anhhoang_chu/iceberg/silver/error_schema\")\n",
    "    .option(\"checkpointLocation\", \"/Volumes/users/anhhoang_chu/iceberg/silver/_error_checkpoint\")\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(\"users.anhhoang_chu.silver_instrument_error\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8908198917773943,
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "127cc629-9aeb-45bc-b8af-16e3d7b0eb4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from users.anhhoang_chu.silver_instrument_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "547bf3d8-2729-48fb-87cd-80bf067cc4df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Instrument Risk Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8908198917773943,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b977b69d-e9d8-4abe-a39e-5b6f68adb5fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users.anhhoang_chu.silver_instrument_risk_metric({instrument_risk_schema})\n",
    "USING ICEBERG\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8908198917773943,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57d9ae62-c859-4a3e-926f-cdacd207df32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col \n",
    "\n",
    "error_df = exploded_df.withColumn(\"riskmetric\", explode(col(\"data_item.instrumentriskmetric\"))).select(\"riskmetric.*\")\n",
    "\n",
    "(\n",
    "error_df.writeStream\n",
    "    .queryName(\"silver_instrument_risk_stream\")\n",
    "    .format(\"iceberg\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"schemaLocation\", \"/Volumes/users/anhhoang_chu/iceberg/silver/risk_schema\")\n",
    "    .option(\"checkpointLocation\", \"/Volumes/users/anhhoang_chu/iceberg/silver/_risk_checkpoint\")\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(\"users.anhhoang_chu.silver_instrument_risk_metric\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8908198917773943,
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5e9f3f1-badb-4bd9-8c53-a134aa153810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from users.anhhoang_chu.silver_instrument_risk_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa6c89e4-b20e-4b31-b473-c3251a5d2fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Monitoring\n",
    "Status of each streaming queries can be viewed in SparkUI/Structured Streaming\n",
    "\n",
    "![pulsar-streaming-queries](./pulsar-streaming-queries.jpg)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8908198917771096,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "iceberg-streaming",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
